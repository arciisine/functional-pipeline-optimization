\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{color}
\usepackage{listings}

\usetikzlibrary{arrows,positioning,calc}
\usetikzlibrary{graphs,arrows.meta}

\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{darkblue}{rgb}{.4,.4,0}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, let, const, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this, filter, map, reduce, foreach, some, find, EXEC, TAG},
  ndkeywordstyle=\color{darkblue}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}


\usepackage[margin=1in]{geometry}

\title{Optimization on a Sequence of Functional Transformations}
\author{Timothy Soehnlin}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
There has always been a disconnect between how humans organize computer code, and how compilers optimize it.  Many times the act of creating abstractions adds computational weight to software in order to increase maintainability, testability and decrease overall complexity.  Generally the additional abstractions do not impact performance signficantly due to the frequency with which the abstractions are invoked.  However, code that is executed many times over, would benefit from optimization as the cost of the introduced abstractions are amplified.  

In functional programming it is a common practice to handle loop ($map$, $filter$, $reduce$) as applying a function to each element of the loop.  Via functional composition (or method chaining), multiple iterations are composed together to handle more complex iterations.  This allows for fairly elegant iteration code as well as code that is easy to test.  This abstraction though, comes with the expense of invoking a function per each loop iteration multiplied by the number of transformations per iteration. In addition to the cost of function execution, there is also substantial memory allocations for $filter$ and $map$ operations as they will need to create intermedidate data to pass between functional invocations.  

A common optimization that is encouraged, throughout many programming languages, is to manually convert the functional idioms into a more traditional for-loop.  This involes manually projecting the functional composition into standard procedural code.  While increasing performance and decreasing memory allocations, the overall cost is tied to code maintainability, and increasing surface area for bugs.  

The main difference of manual operation is that it is pitting the programmer's desire (clean, testable code) against the processor's need for tight loops and register references.  

\section{Functional Transformations}

\begin{enumerate}
  \item Recieve text as a string
  \item Split text into a list of words
  \item Remove all words that are 3 characters or less
  \item Convert all remaining items to lower case
  \item Maintain counter for each word 
  \item When counter passes threshold, emit value
\end{enumerate}

Given the above listing, what you can see is that there is a set of filters, transformations, aggreagations that need to be applied in order to produce the desired output.  This results in a sequence of operations that the data will be manipulated through and ultimately producing the correct results.  

This sequence creates the flow of operations that every text block, and word will have to go through.  This form has a few benefits, specifically that each step is clear and easy to verify.  Additionally, because of the modularity of the functions, each step is easy to modify.  This modification will produce a new program with a high certainty of correctness given the isolation from every other step in the sequence.  

Functional isolation and purity ultimately lend themselves to code that is easier to write, easier to maintain, and easier to verify.  

\subsection{Case Study: JavaScript Array}

An implementation of this concept of functional sequences can be found in how JavaScript models arrays.  The array class implements a superset of the following functionality:

  \begin{enumerate}
    \item $forEach(operation)$.  $operation$ represents a generic function whose return value is ignored.  This method will invoke $operation$ on every element of the array.

    \item $map(transform)$.  $transform$ represents a mapping of the array contents.  This method will create a new array of identical size to the input, but with every element mapped through the $transform$ function.

    \item $filter(predicate)$.  $predicate$ represents a mapping of the array elements to a boolean value.  This method will return a new array in which every element that has a $predicate$ invocation that returns $true$ will be in the output.

    \item $reduce(accumulate, initial)$.  $accumulate$ represents a function that receives both $accumulator$ and an array element. Every time $accumulate$ is is invoked with $accumulator$ and an array element, the output of the function is stored as a new value for $accumulate$.  This new value will be used on the next iteration or will be returned if it is the final iteration.

  \end{enumerate}

  Since $map$ and $filter$ both return arrays, this gives way to method chaining. Method chaining is a design pattern in which an operation on an object returns itself or an instance of the same type as the method's owner. Visually, this produces a strong representation of the pseudo code mentioned earlier:
  
  \lstinputlisting[frame=single, language=JavaScript]{code/sample-functional.js} 

  An even clearer version of this code could be derived by declaring all the operators

  \lstinputlisting[frame=single, language=JavaScript]{code/sample-functional-declared.js}

  This final form even allows for predicates/transformations to be used between multiple transformational sequences.  This leads to higher quality of code by decreasing the testing surface, and preventing code duplication. 

\subsection{Other use cases}

This pattern arises in almost every programming language developed in the last 30 years.  The reason for this, is that functional paradigms are seen as beneficial in general, but almost as a necessity in dealing with complex list manipulations.    

\section{Algorithm}

;Flow Diagram / Decision tree

\section{Process}

;Diagram

\section{Implementation}

\section{Results}

\section{Related Work}

\section{Conclusion}

\end{document}