\subsection{Text Analysis}
This algorithm is doing frequency analysis on a large body of text.  The cost of loading the text and splitting into separate words is negated, and the primary operations of this algorithm are basic comparison and object assignment. 

The results of this scenario are interesting as it highlights the performance characteristics between input size and iterations, extremely well.  It is very easy to see how the input size affects the general efficiency of the system vs the number of times something is run, and the general overhead of executing the code.

The input size as it passes between $10$ and $100$, leads to the optimized code out performing the manual form. 