\section{Introduction}
There has always been a disconnect between how humans organize computer code, and how compilers optimize it.  Many times the act of creating abstractions adds computational weight to software in order to increase maintainability, testability and decrease overall complexity.  Generally the additional abstractions do not impact performance signficantly due to the frequency with which the abstractions are invoked.  However, code that is executed many times over, would benefit from optimization as the cost of the introduced abstractions are amplified.  

In functional programming it is a common practice to handle loop ($map$, $filter$, $reduce$) as applying a function to each element of the loop.  Via functional composition (or method chaining), multiple iterations are composed together to handle more complex iterations.  This allows for fairly elegant iteration code as well as code that is easy to test.  This abstraction though, comes with the expense of invoking a function per each loop iteration multiplied by the number of transformations per iteration. In addition to the cost of function execution, there is also substantial memory allocations for $filter$ and $map$ operations as they will need to create intermedidate data to pass between functional invocations.  

A common optimization that is encouraged, throughout many programming languages, is to manually convert the functional idioms into a more traditional for-loop.  This involes manually projecting the functional composition into standard procedural code.  While increasing performance and decreasing memory allocations, the overall cost is tied to code maintainability, and increasing surface area for bugs.  

The main difference of manual operation is that it is pitting the programmer's desire (clean, testable code) against the processor's need for tight loops and register references.  